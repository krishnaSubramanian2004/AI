import numpy as np
from collections import Counter

class DecisionTree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth

    def fit(self, X, y):
        self.n_classes_ = len(set(y))
        self.n_features_ = X.shape[1]
        self.tree_ = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        if n_samples == 0:
            return {'class': None}
        if len(set(y)) == 1:
            return {'class': y[0]}
        if self.max_depth is not None and depth >= self.max_depth:
            return {'class': Counter(y).most_common(1)[0][0]}

        # Find the best split
        best_split = self._best_split(X, y)

        # Recursive splitting
        left_indices = np.where(X[:, best_split['feature_idx']] <= best_split['threshold'])[0]
        right_indices = np.where(X[:, best_split['feature_idx']] > best_split['threshold'])[0]
        left_tree = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)
        right_tree = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)

        return {'feature_idx': best_split['feature_idx'],
                'threshold': best_split['threshold'],
                'left': left_tree,
                'right': right_tree}

    def _best_split(self, X, y):
        n_samples, n_features = X.shape
        if n_samples <= 1:
            return {'feature_idx': None, 'threshold': None}

        # Initialize variables
        best_split = {'feature_idx': None, 'threshold': None}
        best_gain = -1

        # Calculate the information gain for each feature
        for feature_idx in range(n_features):
            thresholds = np.unique(X[:, feature_idx])
            for threshold in thresholds:
                y_left = y[X[:, feature_idx] <= threshold]
                y_right = y[X[:, feature_idx] > threshold]
                gain = self._information_gain(y, y_left, y_right)
                if gain > best_gain:
                    best_gain = gain
                    best_split = {'feature_idx': feature_idx, 'threshold': threshold}

        return best_split

    def _information_gain(self, y, y_left, y_right):
        n = len(y)
        n_left, n_right = len(y_left), len(y_right)
        ent = self._entropy(y)
        ent_left = self._entropy(y_left)
        ent_right = self._entropy(y_right)
        return ent - (n_left / n * ent_left + n_right / n * ent_right)

    def _entropy(self, y):
        _, counts = np.unique(y, return_counts=True)
        probs = counts / len(y)
        return -np.sum(probs * np.log2(probs + 1e-10))

    def predict(self, X):
        return [self._predict_tree(x, self.tree_) for x in X]

    def _predict_tree(self, x, tree):
        if 'class' in tree:
            return tree['class']
        feature_idx = tree['feature_idx']
        threshold = tree['threshold']
        if x[feature_idx] <= threshold:
            return self._predict_tree(x, tree['left'])
        else:
            return self._predict_tree(x, tree['right'])

# Example usage:
if __name__ == '__main__':
    # Example dataset
    X_train = np.array([[0, 0],
                        [0, 1],
                        [1, 0],
                        [1, 1]])
    y_train = np.array([0, 1, 1, 0])

    # Initialize and train the decision tree
    dt = DecisionTree(max_depth=2)
    dt.fit(X_train, y_train)

    # Example prediction
    X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    predictions = dt.predict(X_test)
    print(predictions)  # Output: [0, 1, 1, 0]
